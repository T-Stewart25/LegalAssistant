{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V 1.3\n",
    "## Focusing on retaining proper citations of data for RAG retreval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: MISTRAL_API_KEY=4yGIhM0TLjmlEr7to6bIhXO7zxUwmljj\n",
      "env: MISTRAL_API_KEY2=wGbVnh3LXp9x18Bk7jvGjoKAwktOBUPj\n",
      "env: SUPABASE_API_KEY=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6InVzeWhxc2ZzcXZlYXRmbmR5Z3NyIiwicm9sZSI6ImFub24iLCJpYXQiOjE3NDI4NjU5MjcsImV4cCI6MjA1ODQ0MTkyN30.7PS-h86deBlBelYG4LbjMk8l3_ZO6AJGhd0dLuNzqlQ\n",
      "env: SUPABASE_API_KEY_2=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6InV4ZHZkb3RvemhkeWpjeXptb3NkIiwicm9sZSI6ImFub24iLCJpYXQiOjE3NDI4NjY3NTgsImV4cCI6MjA1ODQ0Mjc1OH0.BTN_LPXsJ17yl6SIJUPzgh2dfkwIwKPmcYWW0wOQvrg\n",
      "env: VECTORIZE_API_KEY=eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpYXQiOjE3NDM3MTMwMjksImF1ZCI6ImMzMTRlZWY0LTNhNDgtNGVkZS05OGU5LTEwMDQyZjNmZDY1YiIsInJvbGUiOiJhZG1pbiIsImF1dGhvcml6YXRpb25fZGV0YWlscyI6W3sibmFtZSI6IlJFVFJJRVZBTF9BQ0NFU1NfVE9LRU4iLCJpc1N0YW5kYXJkUm9sZSI6dHJ1ZSwicGVybWlzc2lvbnMiOnsiVmVyc2lvbiI6IjEuMCIsIlN0YXRlbWVudCI6W3siQWN0aW9uIjpbIk9yZzpQaXBlbGluZXM6UmV0cmlldmFsIl0sIlJlc291cmNlIjpbIi9vcmdhbml6YXRpb24vYzMxNGVlZjQtM2E0OC00ZWRlLTk4ZTktMTAwNDJmM2ZkNjViIl0sIkVmZmVjdCI6IkFsbG93In1dfX1dLCJzdWIiOiJDbGludCBJbnB1dCJ9.YnvUvp6DCOjFIpE-xrv8Y375nVySkBXGHpUfbo1AC5_Lwa1Bb2bgn_M-vnQzdVezD08367kcZzM2r-YHgDyvrrwThm_kyKRw59zcKBm6J9e-DsjgIFr8Rlys10bbtTKoQr-TaXDnY3NZU5GxxzfN3qf_R7d5R6X8lxXdzVIUUSPjBAUZp7qPCKeuHCzlenMhsrvJWPmSSLlRvLD1zuUBF4MeXh50iU7y608NvbECeQAxmXIsFJq094o8jvBxW_2ELXiPBMCIS42TEUsXHJiwjwz2FBe9Svj821Wii4tjg1eKXq4p3BFxpEVsrpgJlXeqRwNigz8a6zT7Uz8MBjWEew\n",
      "env: VECTORIZE_ORG_KEY=c314eef4-3a48-4ede-98e9-10042f3fd65b\n"
     ]
    }
   ],
   "source": [
    "#!pip install pdf2image pytesseract pillow mistralai tqdm supabase\n",
    "#!pip install typing_extensions==4.10.0 --force-reinstall\n",
    "#!pip install torch torchvision torchaudio\n",
    "#!pip install transformers datasets\n",
    "#!pip install easyocr\n",
    "#!pip install vectorize-client --upgrade\n",
    "\n",
    "\n",
    "#!brew install poppler\n",
    "#!pdfinfo -v\n",
    "#!pip install pdfminer.six pdf2image pytesseract pillow beautifulsoup4\n",
    "#!brew install tesseract  # or apt install tesseract-ocr\n",
    "#!brew install poppler\n",
    "\n",
    "%env MISTRAL_API_KEY=4yGIhM0TLjmlEr7to6bIhXO7zxUwmljj\n",
    "%env MISTRAL_API_KEY2=wGbVnh3LXp9x18Bk7jvGjoKAwktOBUPj\n",
    "%env SUPABASE_API_KEY=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6InVzeWhxc2ZzcXZlYXRmbmR5Z3NyIiwicm9sZSI6ImFub24iLCJpYXQiOjE3NDI4NjU5MjcsImV4cCI6MjA1ODQ0MTkyN30.7PS-h86deBlBelYG4LbjMk8l3_ZO6AJGhd0dLuNzqlQ\n",
    "%env SUPABASE_API_KEY_2=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6InV4ZHZkb3RvemhkeWpjeXptb3NkIiwicm9sZSI6ImFub24iLCJpYXQiOjE3NDI4NjY3NTgsImV4cCI6MjA1ODQ0Mjc1OH0.BTN_LPXsJ17yl6SIJUPzgh2dfkwIwKPmcYWW0wOQvrg\n",
    "%env VECTORIZE_API_KEY=eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpYXQiOjE3NDM3MTMwMjksImF1ZCI6ImMzMTRlZWY0LTNhNDgtNGVkZS05OGU5LTEwMDQyZjNmZDY1YiIsInJvbGUiOiJhZG1pbiIsImF1dGhvcml6YXRpb25fZGV0YWlscyI6W3sibmFtZSI6IlJFVFJJRVZBTF9BQ0NFU1NfVE9LRU4iLCJpc1N0YW5kYXJkUm9sZSI6dHJ1ZSwicGVybWlzc2lvbnMiOnsiVmVyc2lvbiI6IjEuMCIsIlN0YXRlbWVudCI6W3siQWN0aW9uIjpbIk9yZzpQaXBlbGluZXM6UmV0cmlldmFsIl0sIlJlc291cmNlIjpbIi9vcmdhbml6YXRpb24vYzMxNGVlZjQtM2E0OC00ZWRlLTk4ZTktMTAwNDJmM2ZkNjViIl0sIkVmZmVjdCI6IkFsbG93In1dfX1dLCJzdWIiOiJDbGludCBJbnB1dCJ9.YnvUvp6DCOjFIpE-xrv8Y375nVySkBXGHpUfbo1AC5_Lwa1Bb2bgn_M-vnQzdVezD08367kcZzM2r-YHgDyvrrwThm_kyKRw59zcKBm6J9e-DsjgIFr8Rlys10bbtTKoQr-TaXDnY3NZU5GxxzfN3qf_R7d5R6X8lxXdzVIUUSPjBAUZp7qPCKeuHCzlenMhsrvJWPmSSLlRvLD1zuUBF4MeXh50iU7y608NvbECeQAxmXIsFJq094o8jvBxW_2ELXiPBMCIS42TEUsXHJiwjwz2FBe9Svj821Wii4tjg1eKXq4p3BFxpEVsrpgJlXeqRwNigz8a6zT7Uz8MBjWEew\n",
    "%env VECTORIZE_ORG_KEY=c314eef4-3a48-4ede-98e9-10042f3fd65b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Standard Library ===\n",
    "import os\n",
    "import io\n",
    "import re\n",
    "import json\n",
    "import uuid\n",
    "import math\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# === PDF Handling ===\n",
    "from pdf2image import convert_from_path\n",
    "from pdfminer.high_level import extract_text_to_fp\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import HTMLConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.pdfparser import PDFParser\n",
    "from pdfminer.pdfdocument import PDFDocument\n",
    "\n",
    "# === Image Processing ===\n",
    "from PIL import Image, ImageEnhance, ImageFilter\n",
    "\n",
    "# === OCR Libraries ===\n",
    "import pytesseract\n",
    "import torch\n",
    "import easyocr\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "\n",
    "# === HTML Parsing ===\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# === API Clients ===\n",
    "from mistralai import Mistral\n",
    "from supabase import create_client, Client\n",
    "import vectorize_client as v\n",
    "\n",
    "# === Networking (Optional) ===\n",
    "import urllib3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === File CONFIGURATION ===\n",
    "PDF_PATH = \"/Users/thomasstewart/Desktop/DisabilityLawFirm/data/1F_A1001001A23I13B94303C44001.pdf\"\n",
    "RESULT_PATH = \"/Users/thomasstewart/Desktop/DisabilityLawFirm/OutputData/final_output.txt\"\n",
    "FOLDER_PATH = \"/Users/thomasstewart/Desktop/DisabilityLawFirm/OutputData\"\n",
    "JSON_FOLDER_PATH = \"/Users/thomasstewart/Desktop/DisabilityLawFirm/JSONdata\"\n",
    "OUTPUT_JSON_PATH = \"/Users/thomasstewart/Desktop/DisabilityLawFirm/OutputData/api_results.json\"\n",
    "OUTPUT_STATS_PATH = \"/Users/thomasstewart/Desktop/DisabilityLawFirm/OutputData/api_stats.json\"\n",
    "LIVE_OUTPUT_PATH = \"/Users/thomasstewart/Desktop/DisabilityLawFirm/OutputData/live_client_info.jsonl\"\n",
    "DATABASE_PATH = \"/Users/thomasstewart/Desktop/DisabilityLawFirm/DataForDatabase\"\n",
    "\n",
    "# === CLIENT Configuration ===\n",
    "last4_ssn = \"1234\"\n",
    "last_name = \"Voss\"\n",
    "client_id = f\"{last4_ssn}_{last_name}\"\n",
    "\n",
    "# === Summary Configuration ===\n",
    "WAIT_TIME_SECONDS = 100\n",
    "MAX_CHUNK_SIZE = 10000\n",
    "api_keys = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline for Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/prod/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Loading OCR models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config of the encoder: <class 'transformers.models.vit.modeling_vit.ViTModel'> is overwritten by shared encoder config: ViTConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"image_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"pooler_act\": \"tanh\",\n",
      "  \"pooler_output_size\": 768,\n",
      "  \"qkv_bias\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.50.3\"\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'transformers.models.trocr.modeling_trocr.TrOCRForCausalLM'> is overwritten by shared decoder config: TrOCRConfig {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"cross_attention_hidden_size\": 768,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"layernorm_embedding\": true,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"trocr\",\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.50.3\",\n",
      "  \"use_cache\": false,\n",
      "  \"use_learned_position_embeddings\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-handwritten and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Config of the encoder: <class 'transformers.models.vit.modeling_vit.ViTModel'> is overwritten by shared encoder config: ViTConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"image_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"pooler_act\": \"tanh\",\n",
      "  \"pooler_output_size\": 768,\n",
      "  \"qkv_bias\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.50.3\"\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'transformers.models.trocr.modeling_trocr.TrOCRForCausalLM'> is overwritten by shared decoder config: TrOCRConfig {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"cross_attention_hidden_size\": 768,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"layernorm_embedding\": true,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"trocr\",\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.50.3\",\n",
      "  \"use_cache\": false,\n",
      "  \"use_learned_position_embeddings\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-printed and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using CPU. Note: This module is much faster with a GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Models loaded. Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# === Model Initialization ===\n",
    "print(\"🔧 Loading OCR models...\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "trocr_processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
    "trocr_hand = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-handwritten\").to(device)\n",
    "trocr_print = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-printed\").to(device)\n",
    "easyocr_reader = easyocr.Reader(['en'], gpu=torch.cuda.is_available())\n",
    "\n",
    "print(f\"✅ Models loaded. Using device: {device}\")\n",
    "\n",
    "# === Preprocessing Utilities ===\n",
    "def clean_output_folder(folder_path):\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        try:\n",
    "            if os.path.isfile(file_path):\n",
    "                os.remove(file_path)\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"⚠️ Failed to delete {file_path}: {e}\")\n",
    "    print(\"🧹 Output folder cleaned.\")\n",
    "\n",
    "def upscale_image(img, scale_factor=2):\n",
    "    w, h = img.size\n",
    "    return img.resize((w * scale_factor, h * scale_factor), Image.BICUBIC)\n",
    "\n",
    "def enhance_image(img: Image.Image, contrast=2.0) -> Image.Image:\n",
    "    img = img.convert(\"L\")\n",
    "    img = ImageEnhance.Contrast(img).enhance(contrast)\n",
    "    return img.convert(\"RGB\")\n",
    "\n",
    "# === OCR Function ===\n",
    "def run_ocr_with_dual_pass(img: Image.Image) -> str:\n",
    "    output_lines = set()\n",
    "    base_img = upscale_image(img, scale_factor=2)\n",
    "\n",
    "    for label, model in [(\"Handwritten\", trocr_hand), (\"Printed\", trocr_print)]:\n",
    "        try:\n",
    "            input_img = enhance_image(base_img, contrast=2.0)\n",
    "            inputs = trocr_processor(images=input_img, return_tensors=\"pt\").pixel_values.to(device)\n",
    "            ids = model.generate(inputs)\n",
    "            text = trocr_processor.batch_decode(ids, skip_special_tokens=True)[0]\n",
    "            if text.strip() and text.strip() not in [\"0\", \"0 0\"]:\n",
    "                output_lines.add(f\"[TrOCR - {label}]\\n{text.strip()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ TrOCR ({label}) failed: {e}\")\n",
    "\n",
    "    for contrast in [1.5, 2.0, 2.5]:\n",
    "        try:\n",
    "            variant_img = enhance_image(base_img, contrast=contrast)\n",
    "            np_img = np.array(variant_img)\n",
    "            results = easyocr_reader.readtext(np_img)\n",
    "            combined = \"\\n\".join([text for _, text, _ in results])\n",
    "            if combined.strip():\n",
    "                output_lines.add(f\"[EasyOCR - Contrast {contrast}]\\n{combined.strip()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ EasyOCR (contrast {contrast}) failed: {e}\")\n",
    "\n",
    "    return \"\\n\\n\".join(sorted(output_lines)) if output_lines else \"[OCR ERROR] No readable text\"\n",
    "\n",
    "# === Per-Page HTML Extraction ===\n",
    "def extract_per_page_html(pdf_path):\n",
    "    html_pages = []\n",
    "    with open(pdf_path, 'rb') as f:\n",
    "        parser = PDFParser(f)\n",
    "        doc = PDFDocument(parser)\n",
    "        for i, page in enumerate(PDFPage.create_pages(doc)):\n",
    "            output = io.BytesIO()\n",
    "            laparams = LAParams()\n",
    "            resource_manager = PDFResourceManager()\n",
    "            device = HTMLConverter(resource_manager, output, laparams=laparams)\n",
    "            interpreter = PDFPageInterpreter(resource_manager, device)\n",
    "            interpreter.process_page(page)\n",
    "            device.close()\n",
    "            html_content = output.getvalue().decode(\"utf-8\")\n",
    "            html_pages.append(html_content)\n",
    "            output.close()\n",
    "    return html_pages\n",
    "\n",
    "# === Main Pipeline ===\n",
    "def process_pdf_to_clean_text(pdf_path, output_text_path):\n",
    "    file_name = os.path.basename(pdf_path)\n",
    "    print(f\"📄 Processing PDF: {file_name}\")\n",
    "\n",
    "    print(\"📄 Extracting HTML per page...\")\n",
    "    html_pages = extract_per_page_html(pdf_path)\n",
    "\n",
    "    print(\"🧠 Converting PDF to images for OCR...\")\n",
    "    images = convert_from_path(pdf_path, dpi=300)\n",
    "\n",
    "    if len(html_pages) != len(images):\n",
    "        print(\"⚠️ Warning: Mismatch between HTML pages and OCR images!\")\n",
    "\n",
    "    print(\"💾 Writing combined output to file...\")\n",
    "    with open(output_text_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for i, (html, img) in enumerate(zip(html_pages, images)):\n",
    "            page_num = i + 1\n",
    "            f.write(f\"\\n=== START OF PAGE {page_num} ON PDF {file_name} ===\\n\")\n",
    "\n",
    "            # HTML Extraction\n",
    "            f.write(\"--- EMBEDDED TEXT ---\\n\")\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "            lines = [t.strip() for t in soup.find_all(string=True) if t.strip()]\n",
    "            f.write(\"\\n\".join(lines))\n",
    "            f.write(\"\\n\\n\")\n",
    "\n",
    "            # OCR\n",
    "            f.write(\"--- OCR TEXT ---\\n\")\n",
    "            print(f\"🔍 Running OCR on page {page_num}...\")\n",
    "            try:\n",
    "                ocr_text = run_ocr_with_dual_pass(img)\n",
    "                f.write(ocr_text.strip() + \"\\n\")\n",
    "            except Exception as e:\n",
    "                print(f\"❌ OCR failed on page {page_num}: {e}\")\n",
    "                f.write(\"[OCR FAILURE] \" + str(e) + \"\\n\")\n",
    "\n",
    "            f.write(f\"=== END OF PAGE {page_num} ON PDF {file_name} ===\\n\")\n",
    "\n",
    "    print(f\"✅ Final text saved to: {output_text_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a single .txt file containing all text from PDF with multiple OCR readings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧹 Output folder cleaned.\n",
      "🧹 Output folder cleaned.\n",
      "📄 Processing PDF: 1F_A1001001A23I13B94303C44001.pdf\n",
      "📄 Extracting HTML per page...\n",
      "🧠 Converting PDF to images for OCR...\n",
      "💾 Writing combined output to file...\n",
      "🔍 Running OCR on page 1...\n",
      "🔍 Running OCR on page 2...\n",
      "🔍 Running OCR on page 3...\n",
      "🔍 Running OCR on page 4...\n",
      "🔍 Running OCR on page 5...\n",
      "🔍 Running OCR on page 6...\n",
      "🔍 Running OCR on page 7...\n",
      "🔍 Running OCR on page 8...\n",
      "🔍 Running OCR on page 9...\n",
      "🔍 Running OCR on page 10...\n",
      "🔍 Running OCR on page 11...\n",
      "🔍 Running OCR on page 12...\n",
      "🔍 Running OCR on page 13...\n",
      "🔍 Running OCR on page 14...\n",
      "🔍 Running OCR on page 15...\n",
      "🔍 Running OCR on page 16...\n",
      "🔍 Running OCR on page 17...\n",
      "🔍 Running OCR on page 18...\n",
      "🔍 Running OCR on page 19...\n",
      "🔍 Running OCR on page 20...\n",
      "🔍 Running OCR on page 21...\n",
      "🔍 Running OCR on page 22...\n",
      "🔍 Running OCR on page 23...\n",
      "🔍 Running OCR on page 24...\n",
      "🔍 Running OCR on page 25...\n",
      "🔍 Running OCR on page 26...\n",
      "🔍 Running OCR on page 27...\n",
      "🔍 Running OCR on page 28...\n",
      "✅ Final text saved to: /Users/thomasstewart/Desktop/DisabilityLawFirm/OutputData/final_output.txt\n"
     ]
    }
   ],
   "source": [
    "# === INITIALIZE AND RUN PIPELINE ===\n",
    "clean_output_folder(FOLDER_PATH)\n",
    "clean_output_folder(JSON_FOLDER_PATH)\n",
    "clean_output_folder(DATABASE_PATH)\n",
    "text = process_pdf_to_clean_text(\n",
    "    pdf_path=PDF_PATH,\n",
    "    output_text_path=RESULT_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Estimate for PDF to JSON API summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Estimated API Requests Needed: 28\n",
      "⏱️ Wait Time per Request: 100 seconds\n",
      "🕒 Estimated Total Time including 100s API buffer: 1h 10m 0s\n",
      "📄 Total HTML Length: 330222 characters\n",
      "📚 Pages Detected: 28\n"
     ]
    }
   ],
   "source": [
    "# === STEP 1: Load HTML ===\n",
    "with open(RESULT_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    html_text = f.read()\n",
    "\n",
    "# === STEP 2: Split by Page Blocks ===\n",
    "# Match everything between START/END headers that include page number and filename\n",
    "page_chunks = re.findall(\n",
    "    r\"(=== START OF PAGE \\d+ ON PDF .+? ===.*?=== END OF PAGE \\d+ ON PDF .+? ===)\",\n",
    "    html_text,\n",
    "    flags=re.DOTALL\n",
    ")\n",
    "num_requests = len(page_chunks)\n",
    "\n",
    "# === STEP 3: Estimate Time (includes 100   s buffer per request)\n",
    "total_wait_time = (WAIT_TIME_SECONDS * num_requests / api_keys) + (100 * num_requests / api_keys)\n",
    "hours, rem = divmod(total_wait_time, 3600)\n",
    "minutes, seconds = divmod(rem, 60)\n",
    "\n",
    "# === STEP 4: Summary Report ===\n",
    "print(f\"📦 Estimated API Requests Needed: {num_requests}\")\n",
    "print(f\"⏱️ Wait Time per Request: {WAIT_TIME_SECONDS} seconds\")\n",
    "print(f\"🕒 Estimated Total Time including 100s API buffer: {int(hours)}h {int(minutes)}m {int(seconds)}s\")\n",
    "print(f\"📄 Total HTML Length: {len(html_text)} characters\")\n",
    "print(f\"📚 Pages Detected: {num_requests}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reads .txt file and splits data based on page to have Mistral process and provide a summary JSON in OutputData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Worker 1:   0%|                                          | 0/14 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Worker 2:   0%|                                          | 0/14 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "Worker 1:   7%|██▍                               | 1/14 [00:15<03:20, 15.41s/it]\u001b[A\n",
      "\n",
      "Worker 2:   7%|██▍                               | 1/14 [00:29<06:28, 29.87s/it]\u001b[A\u001b[A\n",
      "Worker 1:  14%|████▊                             | 2/14 [02:29<17:03, 85.26s/it]\u001b[A\n",
      "\n",
      "Worker 2:  14%|████▊                             | 2/14 [02:47<18:38, 93.17s/it]\u001b[A\u001b[A\n",
      "Worker 1:  21%|███████▎                          | 3/14 [04:26<18:17, 99.78s/it]\u001b[A\n",
      "\n",
      "Worker 2:  21%|███████                          | 3/14 [04:53<19:50, 108.20s/it]\u001b[A\u001b[A\n",
      "Worker 1:  29%|█████████▍                       | 4/14 [06:22<17:41, 106.10s/it]\u001b[A\n",
      "\n",
      "Worker 2:  29%|█████████▍                       | 4/14 [06:57<19:03, 114.34s/it]\u001b[A\u001b[A\n",
      "Worker 1:  36%|███████████▊                     | 5/14 [08:30<17:05, 114.00s/it]\u001b[A\n",
      "\n",
      "Worker 2:  36%|███████████▊                     | 5/14 [09:07<18:01, 120.13s/it]\u001b[A\u001b[A\n",
      "Worker 1:  43%|██████████████▏                  | 6/14 [10:29<15:25, 115.75s/it]\u001b[A\n",
      "\n",
      "Worker 2:  43%|██████████████▏                  | 6/14 [11:15<16:22, 122.82s/it]\u001b[A\u001b[A\n",
      "Worker 1:  50%|████████████████▌                | 7/14 [12:28<13:38, 116.93s/it]\u001b[A\n",
      "\n",
      "Worker 2:  50%|████████████████▌                | 7/14 [13:20<14:24, 123.51s/it]\u001b[A\u001b[A\n",
      "Worker 1:  57%|██████████████████▊              | 8/14 [14:31<11:52, 118.80s/it]\u001b[A\n",
      "\n",
      "Worker 2:  57%|██████████████████▊              | 8/14 [15:23<12:19, 123.20s/it]\u001b[A\u001b[A\n",
      "Worker 1:  64%|█████████████████████▏           | 9/14 [16:31<09:55, 119.02s/it]\u001b[A\n",
      "\n",
      "Worker 2:  64%|█████████████████████▏           | 9/14 [17:25<10:15, 123.02s/it]\u001b[A\u001b[A\n",
      "Worker 1:  71%|██████████████████████▊         | 10/14 [18:41<08:09, 122.42s/it]\u001b[A\n",
      "\n",
      "Worker 2:  71%|██████████████████████▊         | 10/14 [19:30<08:14, 123.62s/it]\u001b[A\u001b[A\n",
      "Worker 1:  79%|█████████████████████████▏      | 11/14 [20:44<06:08, 122.78s/it]\u001b[A\n",
      "\n",
      "Worker 2:  79%|█████████████████████████▏      | 11/14 [21:49<06:24, 128.20s/it]\u001b[A\u001b[A\n",
      "Worker 1:  86%|███████████████████████████▍    | 12/14 [22:41<04:01, 120.85s/it]\u001b[A\n",
      "\n",
      "Worker 2:  86%|███████████████████████████▍    | 12/14 [23:46<04:09, 124.93s/it]\u001b[A\u001b[A\n",
      "Worker 1:  93%|█████████████████████████████▋  | 13/14 [24:36<01:59, 119.13s/it]\u001b[A\n",
      "\n",
      "Worker 2:  93%|█████████████████████████████▋  | 13/14 [25:41<02:01, 121.99s/it]\u001b[A\u001b[A\n",
      "Worker 1: 100%|████████████████████████████████| 14/14 [26:35<00:00, 119.24s/it]\u001b[A\n",
      "\n",
      "Worker 1: 100%|████████████████████████████████| 14/14 [28:15<00:00, 121.14s/it]\u001b[A\u001b[A\n",
      "Worker 2: 100%|████████████████████████████████| 14/14 [29:21<00:00, 125.85s/it]\n",
      "✅ All results saved to /Users/thomasstewart/Desktop/DisabilityLawFirm/OutputData/api_results.json\n",
      "📡 Live log written to /Users/thomasstewart/Desktop/DisabilityLawFirm/OutputData/live_client_info.jsonl\n",
      "📊 Stats written to /Users/thomasstewart/Desktop/DisabilityLawFirm/OutputData/api_stats.json\n"
     ]
    }
   ],
   "source": [
    "# === Use f-string to dynamically insert variables into the shell command ===\n",
    "command = f\"\"\"\n",
    "python3 summarize_data.py \\\n",
    "  --input_text \"{RESULT_PATH}\" \\\n",
    "  --output_json \"{OUTPUT_JSON_PATH}\" \\\n",
    "  --output_stats \"{OUTPUT_STATS_PATH}\" \\\n",
    "  --live_output \"{LIVE_OUTPUT_PATH}\" \\\n",
    "  --wait_time {WAIT_TIME_SECONDS}\n",
    "\"\"\"\n",
    "\n",
    "# Run the shell command using !\n",
    "!{command}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reads output json in OutputData and provides multiple JSONs to summarize data in JSONdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 Saved section to: /Users/thomasstewart/Desktop/DisabilityLawFirm/JSONdata/Clients.name.json\n",
      "📁 Saved section to: /Users/thomasstewart/Desktop/DisabilityLawFirm/JSONdata/Clients.birth_date.json\n",
      "📁 Saved section to: /Users/thomasstewart/Desktop/DisabilityLawFirm/JSONdata/PersonalIdentifyingInformation.name.json\n",
      "📁 Saved section to: /Users/thomasstewart/Desktop/DisabilityLawFirm/JSONdata/PersonalIdentifyingInformation.birth_date.json\n",
      "📁 Saved section to: /Users/thomasstewart/Desktop/DisabilityLawFirm/JSONdata/MedicalHistory.diagnosis.json\n",
      "📁 Saved section to: /Users/thomasstewart/Desktop/DisabilityLawFirm/JSONdata/MedicalHistory.functional_assessments.json\n",
      "📁 Saved section to: /Users/thomasstewart/Desktop/DisabilityLawFirm/JSONdata/MedicalHistory.physicians.json\n",
      "📁 Saved section to: /Users/thomasstewart/Desktop/DisabilityLawFirm/JSONdata/PersonalIdentifyingInformation.social_security_number.json\n",
      "📁 Saved section to: /Users/thomasstewart/Desktop/DisabilityLawFirm/JSONdata/MedicalHistory.treatment.json\n",
      "📁 Saved section to: /Users/thomasstewart/Desktop/DisabilityLawFirm/JSONdata/MedicalHistory.medications.json\n",
      "📁 Saved section to: /Users/thomasstewart/Desktop/DisabilityLawFirm/JSONdata/MedicalHistory.imaging_results.json\n",
      "📁 Saved section to: /Users/thomasstewart/Desktop/DisabilityLawFirm/JSONdata/WorkHistory.job_title.json\n",
      "📁 Saved section to: /Users/thomasstewart/Desktop/DisabilityLawFirm/JSONdata/WorkHistory.reason_for_leaving.json\n",
      "📁 Saved section to: /Users/thomasstewart/Desktop/DisabilityLawFirm/JSONdata/EducationHistory.field_of_study.json\n",
      "📁 Saved section to: /Users/thomasstewart/Desktop/DisabilityLawFirm/JSONdata/MedicalHistory.allergies.json\n",
      "📁 Saved section to: /Users/thomasstewart/Desktop/DisabilityLawFirm/JSONdata/MedicalHistory.surgeries.json\n",
      "📁 Saved section to: /Users/thomasstewart/Desktop/DisabilityLawFirm/JSONdata/Clients.section.json\n",
      "📁 Saved section to: /Users/thomasstewart/Desktop/DisabilityLawFirm/JSONdata/MedicalHistory.therapists.json\n",
      "📁 Saved section to: /Users/thomasstewart/Desktop/DisabilityLawFirm/JSONdata/MedicalVisits.json\n",
      "✅ RAG-ready JSON saved to: /Users/thomasstewart/Desktop/DisabilityLawFirm/OutputData/final_rag_friendly_output.json\n"
     ]
    }
   ],
   "source": [
    "# === FILE CONFIGURATION ===\n",
    "RESPONSE_FILE = OUTPUT_JSON_PATH\n",
    "SPLIT_OUTPUT_DIR = JSON_FOLDER_PATH\n",
    "\n",
    "FINAL_JSON_PATH = os.path.join(FOLDER_PATH, \"final_consolidated_output.json\")\n",
    "VALUE_COUNT_PATH = os.path.join(FOLDER_PATH, \"field_value_counts.json\")\n",
    "EMBED_READY_JSON_PATH = os.path.join(FOLDER_PATH, \"final_rag_friendly_output.json\")\n",
    "\n",
    "os.makedirs(SPLIT_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "MULTI_VALUE_FIELDS = {\n",
    "    \"medications\", \"allergies\", \"therapists\", \"physicians\", \"surgeries\",\n",
    "    \"hospitalizations\", \"diagnosis\", \"lab_results\", \"imaging_results\", \"functional_assessments\",\n",
    "    \"colleagues\", \"job_duties\", \"assets\", \"liabilities\", \"bank_statements\", \"expenses\", \"courses_taken\"\n",
    "}\n",
    "\n",
    "field_values = defaultdict(list)\n",
    "medical_visits = []\n",
    "\n",
    "# === Utilities ===\n",
    "def flatten_response(response_dict):\n",
    "    flat_data = {}\n",
    "    for section, fields in response_dict.items():\n",
    "        if isinstance(fields, dict):\n",
    "            for key, val in fields.items():\n",
    "                flat_data[f\"{section}.{key}\"] = val\n",
    "    return flat_data\n",
    "\n",
    "def parse_date_safe(date_str):\n",
    "    if not date_str:\n",
    "        return None\n",
    "    for fmt in (\"%Y-%m-%d\", \"%m/%d/%Y\", \"%Y/%m/%d\"):\n",
    "        try:\n",
    "            return datetime.strptime(date_str, fmt)\n",
    "        except:\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "# === Load and Process Responses with Hardcoded Filename Citations ===\n",
    "PDF_PATH = \"/Users/thomasstewart/Desktop/DisabilityLawFirm/data/1F_A1001001A23I13B94303C44001.pdf\"\n",
    "true_file_name = os.path.basename(PDF_PATH)\n",
    "\n",
    "with open(RESPONSE_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    responses = json.load(f)\n",
    "\n",
    "for entry in responses:\n",
    "    try:\n",
    "        content = entry[\"content\"]\n",
    "        parsed = content if isinstance(content, dict) else json.loads(content)\n",
    "        page_number = entry.get(\"page_number\") or parsed.get(\"page_number\")\n",
    "\n",
    "        flat = flatten_response(parsed)\n",
    "        for field, value in flat.items():\n",
    "            if not value:\n",
    "                continue\n",
    "            key = field.split(\".\")[1]\n",
    "            if key in MULTI_VALUE_FIELDS and isinstance(value, str):\n",
    "                items = [v.strip() for v in value.split(\",\") if v.strip()]\n",
    "                for item in items:\n",
    "                    if isinstance(item, dict):\n",
    "                        item = json.dumps(item, sort_keys=True)\n",
    "                    field_values[field].append({\"value\": str(item), \"source\": {\"file\": true_file_name, \"page\": page_number}})\n",
    "            else:\n",
    "                if isinstance(value, dict):\n",
    "                    value = json.dumps(value, sort_keys=True)\n",
    "                field_values[field].append({\"value\": str(value), \"source\": {\"file\": true_file_name, \"page\": page_number}})\n",
    "\n",
    "        for visit in parsed.get(\"MedicalVisits\", []):\n",
    "            if isinstance(visit, dict) and any(visit.values()):\n",
    "                visit[\"source\"] = {\"file\": true_file_name, \"page\": page_number}\n",
    "                medical_visits.append(visit)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Skipping entry due to error: {e}\")\n",
    "\n",
    "# === Final Consolidated JSON ===\n",
    "final_summary = {field: entries for field, entries in field_values.items()}\n",
    "final_summary[\"MedicalVisits\"] = sorted(medical_visits, key=lambda v: parse_date_safe(v.get(\"date\")) or datetime.min)\n",
    "\n",
    "with open(FINAL_JSON_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(final_summary, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# === Field Value Count for Debugging ===\n",
    "value_counts = defaultdict(lambda: defaultdict(int))\n",
    "for field, entries in field_values.items():\n",
    "    for entry in entries:\n",
    "        value = entry[\"value\"]\n",
    "        try:\n",
    "            key = json.dumps(value, sort_keys=True) if isinstance(value, dict) else str(value)\n",
    "            value_counts[field][key] += 1\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Could not count value for {field}: {e}\")\n",
    "\n",
    "with open(VALUE_COUNT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(value_counts, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# === RAG-Ready Friendly Embedding JSON ===\n",
    "def simplify_for_embedding(data):\n",
    "    simplified = defaultdict(list)\n",
    "    for field, entries in data.items():\n",
    "        if field == \"MedicalVisits\":\n",
    "            for visit in entries:\n",
    "                summary = \"; \".join(f\"{k}: {v}\" for k, v in visit.items() if k != \"source\" and v)\n",
    "                citation = f\"(page {visit['source']['page']}, {visit['source']['file']})\"\n",
    "                simplified[field].append(f\"{summary} {citation}\")\n",
    "        else:\n",
    "            grouped = defaultdict(list)\n",
    "            for item in entries:\n",
    "                src = item[\"source\"]\n",
    "                key = (src[\"file\"], src[\"page\"])\n",
    "                grouped[key].append(str(item[\"value\"]))\n",
    "            for (file, page), texts in grouped.items():\n",
    "                combined = \"; \".join(texts)\n",
    "                simplified[field].append(f\"{combined} (page {page}, {file})\")\n",
    "    return simplified\n",
    "\n",
    "rag_friendly = simplify_for_embedding(final_summary)\n",
    "\n",
    "with open(EMBED_READY_JSON_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(rag_friendly, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# === Save Split JSONs by Section (from RAG-friendly JSON) ===\n",
    "for section, entries in rag_friendly.items():\n",
    "    section_path = os.path.join(SPLIT_OUTPUT_DIR, f\"{section}.json\")\n",
    "    with open(section_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(entries, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"📁 Saved section to: {section_path}\")\n",
    "\n",
    "print(f\"✅ RAG-ready JSON saved to: {EMBED_READY_JSON_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reads Organized JSON data in JSONdata and converts it to a format friendly for embedding and eventual RAG retreval and stores restulting insertion queiries in DataForDatabase based on user inputed last name and last 4 of social."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧹 Output folder cleaned.\n",
      "✅ Wrote 1 inserts to /Users/thomasstewart/Desktop/DisabilityLawFirm/DataForDatabase/EducationHistory.field_of_study_insert.json\n",
      "✅ Wrote 3 inserts to /Users/thomasstewart/Desktop/DisabilityLawFirm/DataForDatabase/PersonalIdentifyingInformation.name_insert.json\n",
      "✅ Wrote 3 inserts to /Users/thomasstewart/Desktop/DisabilityLawFirm/DataForDatabase/Clients.name_insert.json\n",
      "✅ Wrote 17 inserts to /Users/thomasstewart/Desktop/DisabilityLawFirm/DataForDatabase/MedicalHistory.functional_assessments_insert.json\n",
      "✅ Wrote 1 inserts to /Users/thomasstewart/Desktop/DisabilityLawFirm/DataForDatabase/PersonalIdentifyingInformation.social_security_number_insert.json\n",
      "✅ Wrote 2 inserts to /Users/thomasstewart/Desktop/DisabilityLawFirm/DataForDatabase/Clients.section_insert.json\n",
      "✅ Wrote 1 inserts to /Users/thomasstewart/Desktop/DisabilityLawFirm/DataForDatabase/WorkHistory.reason_for_leaving_insert.json\n",
      "✅ Wrote 2 inserts to /Users/thomasstewart/Desktop/DisabilityLawFirm/DataForDatabase/MedicalHistory.medications_insert.json\n",
      "✅ Wrote 1 inserts to /Users/thomasstewart/Desktop/DisabilityLawFirm/DataForDatabase/PersonalIdentifyingInformation.birth_date_insert.json\n",
      "✅ Wrote 1 inserts to /Users/thomasstewart/Desktop/DisabilityLawFirm/DataForDatabase/MedicalHistory.allergies_insert.json\n",
      "✅ Wrote 1 inserts to /Users/thomasstewart/Desktop/DisabilityLawFirm/DataForDatabase/MedicalHistory.therapists_insert.json\n",
      "✅ Wrote 22 inserts to /Users/thomasstewart/Desktop/DisabilityLawFirm/DataForDatabase/MedicalVisits_insert.json\n",
      "✅ Wrote 1 inserts to /Users/thomasstewart/Desktop/DisabilityLawFirm/DataForDatabase/Clients.birth_date_insert.json\n",
      "✅ Wrote 5 inserts to /Users/thomasstewart/Desktop/DisabilityLawFirm/DataForDatabase/MedicalHistory.diagnosis_insert.json\n",
      "✅ Wrote 1 inserts to /Users/thomasstewart/Desktop/DisabilityLawFirm/DataForDatabase/MedicalHistory.imaging_results_insert.json\n",
      "✅ Wrote 1 inserts to /Users/thomasstewart/Desktop/DisabilityLawFirm/DataForDatabase/MedicalHistory.treatment_insert.json\n",
      "✅ Wrote 1 inserts to /Users/thomasstewart/Desktop/DisabilityLawFirm/DataForDatabase/MedicalHistory.surgeries_insert.json\n",
      "✅ Wrote 1 inserts to /Users/thomasstewart/Desktop/DisabilityLawFirm/DataForDatabase/WorkHistory.job_title_insert.json\n",
      "✅ Wrote 2 inserts to /Users/thomasstewart/Desktop/DisabilityLawFirm/DataForDatabase/MedicalHistory.physicians_insert.json\n",
      "📦 Total entries written to: /Users/thomasstewart/Desktop/DisabilityLawFirm/DataForDatabase/all_insertions.json\n"
     ]
    }
   ],
   "source": [
    "# === CONFIGURATION ===\n",
    "SOURCE_DIR = JSON_FOLDER_PATH\n",
    "DEST_DIR = DATABASE_PATH\n",
    "os.makedirs(DEST_DIR, exist_ok=True)\n",
    "\n",
    "# === IDENTIFIERS ===\n",
    "CLIENT_ID = f\"{last_name}-{last4_ssn}\"\n",
    "\n",
    "# === UTILITIES ===\n",
    "def extract_page_and_file(text):\n",
    "    match = re.search(r\"\\(page[s]* ([\\d, ]+), ([^)]+)\\)\", text)\n",
    "    if match:\n",
    "        pages = [int(p.strip()) for p in match.group(1).split(\",\")]\n",
    "        file_name = match.group(2).strip()\n",
    "        return pages, file_name\n",
    "    return [], None\n",
    "\n",
    "# === PROCESSING ===\n",
    "def process_json_file(filepath, section_name):\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    def safe_string(value):\n",
    "        return value if isinstance(value, str) else \"\"\n",
    "\n",
    "    insert_objects = defaultdict(lambda: {\n",
    "        \"pages\": set(),\n",
    "        \"file\": \"\",\n",
    "        \"content\": \"\",\n",
    "        \"embedding_input\": \"\",\n",
    "        \"field\": None\n",
    "    })\n",
    "\n",
    "    for item in data:\n",
    "        if isinstance(item, str):\n",
    "            pages, file_name = extract_page_and_file(item)\n",
    "            key = re.sub(r\"\\(page[s]* [^)]+\\)\", \"\", item).strip()\n",
    "            if key:\n",
    "                insert_objects[key][\"content\"] = key\n",
    "                insert_objects[key][\"file\"] = file_name\n",
    "                insert_objects[key][\"embedding_input\"] = f\"{section_name}: {key}\"\n",
    "                insert_objects[key][\"pages\"].update(pages)\n",
    "\n",
    "        elif isinstance(item, dict):\n",
    "            source = item.get(\"source\", {})\n",
    "            file_name = source.get(\"file\", \"\")\n",
    "            page = source.get(\"page\", None)\n",
    "            for key, value in item.items():\n",
    "                if key == \"source\" or not value:\n",
    "                    continue\n",
    "                content_key = f\"{key}::{value}\"\n",
    "                if content_key not in insert_objects:\n",
    "                    insert_objects[content_key][\"field\"] = key\n",
    "                    insert_objects[content_key][\"content\"] = value\n",
    "                    insert_objects[content_key][\"embedding_input\"] = f\"{section_name} - {key}: {value}\"\n",
    "                    insert_objects[content_key][\"file\"] = file_name\n",
    "                if page is not None:\n",
    "                    insert_objects[content_key][\"pages\"].add(page)\n",
    "\n",
    "    # Format to Flowise-friendly vector format\n",
    "    formatted = []\n",
    "    for obj in insert_objects.values():\n",
    "        vector_entry = {\n",
    "            \"id\": f\"{CLIENT_ID}_{section_name}\",\n",
    "            \"text\": safe_string(obj[\"embedding_input\"]),\n",
    "            \"metadata\": {\n",
    "                \"client_id\": CLIENT_ID,\n",
    "                \"section\": section_name,\n",
    "                \"field\": safe_string(obj[\"field\"]),\n",
    "                \"content\": safe_string(obj[\"content\"]),\n",
    "                \"citation\": {\n",
    "                    \"file\": safe_string(obj[\"file\"]),\n",
    "                    \"pages\": sorted(obj[\"pages\"])\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        formatted.append(vector_entry)\n",
    "\n",
    "    return formatted\n",
    "\n",
    "\n",
    "# === MAIN LOOP ===\n",
    "clean_output_folder(DATABASE_PATH)\n",
    "all_insertions = []\n",
    "\n",
    "for filename in os.listdir(SOURCE_DIR):\n",
    "    if filename.endswith(\".json\"):\n",
    "        path = os.path.join(SOURCE_DIR, filename)\n",
    "        section_name = Path(filename).stem\n",
    "        section_inserts = process_json_file(path, section_name)\n",
    "\n",
    "        # Save individual section insert JSON\n",
    "        out_path = os.path.join(DEST_DIR, f\"{section_name}_insert.json\")\n",
    "        with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(section_inserts, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"✅ Wrote {len(section_inserts)} inserts to {out_path}\")\n",
    "\n",
    "        all_insertions.extend(section_inserts)\n",
    "\n",
    "# === COMBINED INSERT FILE ===\n",
    "combined_path = os.path.join(DEST_DIR, \"all_insertions.json\")\n",
    "with open(combined_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_insertions, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"📦 Total entries written to: {combined_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
