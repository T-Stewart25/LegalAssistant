{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting vectorize-client\n",
      "  Downloading vectorize_client-0.1.3-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: pydantic>=2 in /opt/anaconda3/envs/prod/lib/python3.12/site-packages (from vectorize-client) (2.10.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/prod/lib/python3.12/site-packages (from vectorize-client) (2.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.7.1 in /opt/anaconda3/envs/prod/lib/python3.12/site-packages (from vectorize-client) (4.13.0)\n",
      "Requirement already satisfied: urllib3<3.0.0,>=1.25.3 in /opt/anaconda3/envs/prod/lib/python3.12/site-packages (from vectorize-client) (2.2.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/envs/prod/lib/python3.12/site-packages (from pydantic>=2->vectorize-client) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /opt/anaconda3/envs/prod/lib/python3.12/site-packages (from pydantic>=2->vectorize-client) (2.27.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/prod/lib/python3.12/site-packages (from python-dateutil>=2.8.2->vectorize-client) (1.16.0)\n",
      "Downloading vectorize_client-0.1.3-py3-none-any.whl (144 kB)\n",
      "Installing collected packages: vectorize-client\n",
      "Successfully installed vectorize-client-0.1.3\n",
      "env: MISTRAL_API_KEY=4yGIhM0TLjmlEr7to6bIhXO7zxUwmljj\n",
      "env: MISTRAL_API_KEY2=wGbVnh3LXp9x18Bk7jvGjoKAwktOBUPj\n",
      "env: SUPABASE_API_KEY=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6InVzeWhxc2ZzcXZlYXRmbmR5Z3NyIiwicm9sZSI6ImFub24iLCJpYXQiOjE3NDI4NjU5MjcsImV4cCI6MjA1ODQ0MTkyN30.7PS-h86deBlBelYG4LbjMk8l3_ZO6AJGhd0dLuNzqlQ\n",
      "env: SUPABASE_API_KEY_2=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6InV4ZHZkb3RvemhkeWpjeXptb3NkIiwicm9sZSI6ImFub24iLCJpYXQiOjE3NDI4NjY3NTgsImV4cCI6MjA1ODQ0Mjc1OH0.BTN_LPXsJ17yl6SIJUPzgh2dfkwIwKPmcYWW0wOQvrg\n",
      "env: VECTORIZE_API_KEY=eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpYXQiOjE3NDM3MTMwMjksImF1ZCI6ImMzMTRlZWY0LTNhNDgtNGVkZS05OGU5LTEwMDQyZjNmZDY1YiIsInJvbGUiOiJhZG1pbiIsImF1dGhvcml6YXRpb25fZGV0YWlscyI6W3sibmFtZSI6IlJFVFJJRVZBTF9BQ0NFU1NfVE9LRU4iLCJpc1N0YW5kYXJkUm9sZSI6dHJ1ZSwicGVybWlzc2lvbnMiOnsiVmVyc2lvbiI6IjEuMCIsIlN0YXRlbWVudCI6W3siQWN0aW9uIjpbIk9yZzpQaXBlbGluZXM6UmV0cmlldmFsIl0sIlJlc291cmNlIjpbIi9vcmdhbml6YXRpb24vYzMxNGVlZjQtM2E0OC00ZWRlLTk4ZTktMTAwNDJmM2ZkNjViIl0sIkVmZmVjdCI6IkFsbG93In1dfX1dLCJzdWIiOiJDbGludCBJbnB1dCJ9.YnvUvp6DCOjFIpE-xrv8Y375nVySkBXGHpUfbo1AC5_Lwa1Bb2bgn_M-vnQzdVezD08367kcZzM2r-YHgDyvrrwThm_kyKRw59zcKBm6J9e-DsjgIFr8Rlys10bbtTKoQr-TaXDnY3NZU5GxxzfN3qf_R7d5R6X8lxXdzVIUUSPjBAUZp7qPCKeuHCzlenMhsrvJWPmSSLlRvLD1zuUBF4MeXh50iU7y608NvbECeQAxmXIsFJq094o8jvBxW_2ELXiPBMCIS42TEUsXHJiwjwz2FBe9Svj821Wii4tjg1eKXq4p3BFxpEVsrpgJlXeqRwNigz8a6zT7Uz8MBjWEew\n",
      "env: VECTORIZE_ORG_KEY=c314eef4-3a48-4ede-98e9-10042f3fd65b\n"
     ]
    }
   ],
   "source": [
    "#!pip install pdf2image pytesseract pillow mistralai tqdm supabase\n",
    "#!pip install typing_extensions==4.10.0 --force-reinstall\n",
    "#!pip install torch torchvision torchaudio\n",
    "#!pip install transformers datasets\n",
    "#!pip install easyocr\n",
    "!pip install vectorize-client --upgrade\n",
    "%env MISTRAL_API_KEY=4yGIhM0TLjmlEr7to6bIhXO7zxUwmljj\n",
    "%env MISTRAL_API_KEY2=wGbVnh3LXp9x18Bk7jvGjoKAwktOBUPj\n",
    "%env SUPABASE_API_KEY=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6InVzeWhxc2ZzcXZlYXRmbmR5Z3NyIiwicm9sZSI6ImFub24iLCJpYXQiOjE3NDI4NjU5MjcsImV4cCI6MjA1ODQ0MTkyN30.7PS-h86deBlBelYG4LbjMk8l3_ZO6AJGhd0dLuNzqlQ\n",
    "%env SUPABASE_API_KEY_2=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6InV4ZHZkb3RvemhkeWpjeXptb3NkIiwicm9sZSI6ImFub24iLCJpYXQiOjE3NDI4NjY3NTgsImV4cCI6MjA1ODQ0Mjc1OH0.BTN_LPXsJ17yl6SIJUPzgh2dfkwIwKPmcYWW0wOQvrg\n",
    "%env VECTORIZE_API_KEY=eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpYXQiOjE3NDM3MTMwMjksImF1ZCI6ImMzMTRlZWY0LTNhNDgtNGVkZS05OGU5LTEwMDQyZjNmZDY1YiIsInJvbGUiOiJhZG1pbiIsImF1dGhvcml6YXRpb25fZGV0YWlscyI6W3sibmFtZSI6IlJFVFJJRVZBTF9BQ0NFU1NfVE9LRU4iLCJpc1N0YW5kYXJkUm9sZSI6dHJ1ZSwicGVybWlzc2lvbnMiOnsiVmVyc2lvbiI6IjEuMCIsIlN0YXRlbWVudCI6W3siQWN0aW9uIjpbIk9yZzpQaXBlbGluZXM6UmV0cmlldmFsIl0sIlJlc291cmNlIjpbIi9vcmdhbml6YXRpb24vYzMxNGVlZjQtM2E0OC00ZWRlLTk4ZTktMTAwNDJmM2ZkNjViIl0sIkVmZmVjdCI6IkFsbG93In1dfX1dLCJzdWIiOiJDbGludCBJbnB1dCJ9.YnvUvp6DCOjFIpE-xrv8Y375nVySkBXGHpUfbo1AC5_Lwa1Bb2bgn_M-vnQzdVezD08367kcZzM2r-YHgDyvrrwThm_kyKRw59zcKBm6J9e-DsjgIFr8Rlys10bbtTKoQr-TaXDnY3NZU5GxxzfN3qf_R7d5R6X8lxXdzVIUUSPjBAUZp7qPCKeuHCzlenMhsrvJWPmSSLlRvLD1zuUBF4MeXh50iU7y608NvbECeQAxmXIsFJq094o8jvBxW_2ELXiPBMCIS42TEUsXHJiwjwz2FBe9Svj821Wii4tjg1eKXq4p3BFxpEVsrpgJlXeqRwNigz8a6zT7Uz8MBjWEew\n",
    "%env VECTORIZE_ORG_KEY=c314eef4-3a48-4ede-98e9-10042f3fd65b\n",
    "#!brew install poppler\n",
    "#!pdfinfo -v\n",
    "#!pip install pdfminer.six pdf2image pytesseract pillow beautifulsoup4\n",
    "#!brew install tesseract  # or apt install tesseract-ocr\n",
    "#!brew install poppler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Standard Library ===\n",
    "import os\n",
    "import io\n",
    "import json\n",
    "import uuid\n",
    "import math\n",
    "from collections import defaultdict, Counter\n",
    "from datetime import datetime\n",
    "import urllib3, json, os\n",
    "\n",
    "# === PDF and OCR ===\n",
    "from pdf2image import convert_from_path\n",
    "from pdfminer.high_level import extract_text_to_fp\n",
    "from PIL import Image, ImageEnhance, ImageFilter\n",
    "import pytesseract\n",
    "\n",
    "# === HTML Parsing ===\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# === API Client ===\n",
    "from mistralai import Mistral\n",
    "from supabase import create_client, Client\n",
    "import vectorize_client as v\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === File CONFIGURATION ===\n",
    "PDF_PATH = \"/Users/thomasstewart/Desktop/DisabilityLawFirm/data/1F_A1001001A23I13B94303C44001.pdf\"\n",
    "RESULT_PATH = \"/Users/thomasstewart/Desktop/DisabilityLawFirm/OutputData/final_output.txt\"\n",
    "FOLDER_PATH = \"/Users/thomasstewart/Desktop/DisabilityLawFirm/OutputData\"\n",
    "OUTPUT_JSON_PATH = \"/Users/thomasstewart/Desktop/DisabilityLawFirm/OutputData/api_results.json\"\n",
    "OUTPUT_STATS_PATH = \"/Users/thomasstewart/Desktop/DisabilityLawFirm/OutputData/api_stats.json\"\n",
    "LIVE_OUTPUT_PATH = \"/Users/thomasstewart/Desktop/DisabilityLawFirm/OutputData/live_client_info.jsonl\"\n",
    "\n",
    "# === CLIENT Configuration ===\n",
    "last4_ssn = \"1234\"  # Example value; replace with actual input\n",
    "last_name = \"Voss\"  # Example value; replace with actual input\n",
    "client_id = f\"{last4_ssn}_{last_name}\"\n",
    "\n",
    "# === Summary Configuration ===\n",
    "WAIT_TIME_SECONDS = 100\n",
    "MAX_CHUNK_SIZE = 10000\n",
    "api_keys = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline for Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Loading OCR models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config of the encoder: <class 'transformers.models.vit.modeling_vit.ViTModel'> is overwritten by shared encoder config: ViTConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"image_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"pooler_act\": \"tanh\",\n",
      "  \"pooler_output_size\": 768,\n",
      "  \"qkv_bias\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.50.3\"\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'transformers.models.trocr.modeling_trocr.TrOCRForCausalLM'> is overwritten by shared decoder config: TrOCRConfig {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"cross_attention_hidden_size\": 768,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"layernorm_embedding\": true,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"trocr\",\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.50.3\",\n",
      "  \"use_cache\": false,\n",
      "  \"use_learned_position_embeddings\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-handwritten and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Config of the encoder: <class 'transformers.models.vit.modeling_vit.ViTModel'> is overwritten by shared encoder config: ViTConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"image_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"pooler_act\": \"tanh\",\n",
      "  \"pooler_output_size\": 768,\n",
      "  \"qkv_bias\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.50.3\"\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'transformers.models.trocr.modeling_trocr.TrOCRForCausalLM'> is overwritten by shared decoder config: TrOCRConfig {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"cross_attention_hidden_size\": 768,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"layernorm_embedding\": true,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"trocr\",\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.50.3\",\n",
      "  \"use_cache\": false,\n",
      "  \"use_learned_position_embeddings\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-printed and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using CPU. Note: This module is much faster with a GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Models loaded. Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# === Standard Library ===\n",
    "import os\n",
    "import io\n",
    "import numpy as np\n",
    "import logging\n",
    "from collections import defaultdict\n",
    "from pdf2image import convert_from_path\n",
    "from pdfminer.high_level import extract_text_to_fp\n",
    "from PIL import Image, ImageEnhance\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# === OCR Libraries ===\n",
    "import torch\n",
    "import easyocr\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "\n",
    "# === Model Initialization ===\n",
    "print(\"🔧 Loading OCR models...\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# TrOCR - Both Models\n",
    "trocr_processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
    "trocr_hand = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-handwritten\").to(device)\n",
    "trocr_print = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-printed\").to(device)\n",
    "\n",
    "# EasyOCR\n",
    "easyocr_reader = easyocr.Reader(['en'], gpu=torch.cuda.is_available())\n",
    "\n",
    "print(f\"✅ Models loaded. Using device: {device}\")\n",
    "\n",
    "# === Preprocessing Utilities ===\n",
    "def clean_output_folder(folder_path):\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        try:\n",
    "            if os.path.isfile(file_path):\n",
    "                os.remove(file_path)\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"⚠️ Failed to delete {file_path}: {e}\")\n",
    "    print(\"🧹 Output folder cleaned.\")\n",
    "\n",
    "def upscale_image(img, scale_factor=2):\n",
    "    w, h = img.size\n",
    "    return img.resize((w * scale_factor, h * scale_factor), Image.BICUBIC)\n",
    "\n",
    "def enhance_image(img: Image.Image, contrast=2.0) -> Image.Image:\n",
    "    img = img.convert(\"L\")\n",
    "    img = ImageEnhance.Contrast(img).enhance(contrast)\n",
    "    return img.convert(\"RGB\")\n",
    "\n",
    "# === Embedded Text Extraction ===\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    output = io.BytesIO()\n",
    "    with open(pdf_path, 'rb') as f:\n",
    "        extract_text_to_fp(f, output, output_type='html')\n",
    "    return output.getvalue().decode(\"utf-8\")\n",
    "\n",
    "def extract_plain_text_from_html(raw_html):\n",
    "    soup = BeautifulSoup(raw_html, \"html.parser\")\n",
    "    return [t.strip() for t in soup.find_all(string=True) if t.strip()]\n",
    "\n",
    "# === OCR Function ===\n",
    "def run_ocr_with_dual_pass(img: Image.Image) -> str:\n",
    "    \"\"\"Runs TrOCR (both models) + EasyOCR (multiple contrasts) on the image.\"\"\"\n",
    "    output_lines = set()\n",
    "\n",
    "    # Step 1: Preprocess base image\n",
    "    base_img = upscale_image(img, scale_factor=2)\n",
    "\n",
    "    # Step 2: Run TrOCR (Handwritten + Printed)\n",
    "    for label, model in [(\"Handwritten\", trocr_hand), (\"Printed\", trocr_print)]:\n",
    "        try:\n",
    "            input_img = enhance_image(base_img, contrast=2.0)\n",
    "            inputs = trocr_processor(images=input_img, return_tensors=\"pt\").pixel_values.to(device)\n",
    "            ids = model.generate(inputs)\n",
    "            text = trocr_processor.batch_decode(ids, skip_special_tokens=True)[0]\n",
    "            if text.strip() and text.strip() not in [\"0\", \"0 0\"]:\n",
    "                output_lines.add(f\"[TrOCR - {label}]\\n{text.strip()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ TrOCR ({label}) failed: {e}\")\n",
    "\n",
    "    # Step 3: Run EasyOCR at multiple contrast levels\n",
    "    for contrast in [1.5, 2.0, 2.5]:\n",
    "        try:\n",
    "            variant_img = enhance_image(base_img, contrast=contrast)\n",
    "            np_img = np.array(variant_img)\n",
    "            results = easyocr_reader.readtext(np_img)\n",
    "            combined = \"\\n\".join([text for _, text, _ in results])\n",
    "            if combined.strip():\n",
    "                output_lines.add(f\"[EasyOCR - Contrast {contrast}]\\n{combined.strip()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ EasyOCR (contrast {contrast}) failed: {e}\")\n",
    "\n",
    "    return \"\\n\\n\".join(sorted(output_lines)) if output_lines else \"[OCR ERROR] No readable text\"\n",
    "\n",
    "# === OCR for Entire PDF ===\n",
    "def extract_ocr_from_pdf(pdf_path, dpi=300):\n",
    "    images = convert_from_path(pdf_path, dpi=dpi)\n",
    "    ocr_results = []\n",
    "\n",
    "    for i, img in enumerate(images, start=1):\n",
    "        print(f\"🔍 Running OCR on page {i}...\")\n",
    "        try:\n",
    "            ocr_text = run_ocr_with_dual_pass(img)\n",
    "            ocr_results.append(f\"--- OCR TEXT FROM PAGE {i} ---\\n{ocr_text}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed OCR on page {i}: {e}\")\n",
    "            ocr_results.append(f\"--- OCR TEXT FROM PAGE {i} ---\\n[OCR FAILURE] {e}\")\n",
    "\n",
    "    return ocr_results\n",
    "\n",
    "# === Merge Embedded + OCR Text ===\n",
    "def merge_texts(structured_lines, ocr_blocks):\n",
    "    merged = []\n",
    "    if structured_lines:\n",
    "        merged.append(\"--- EMBEDDED TEXT FROM PDF ---\")\n",
    "        merged.extend(structured_lines)\n",
    "    if ocr_blocks:\n",
    "        merged.append(\"--- OCR TEXT FROM SCANNED IMAGES ---\")\n",
    "        merged.extend(ocr_blocks)\n",
    "    return merged\n",
    "\n",
    "# === Main Pipeline ===\n",
    "def process_pdf_to_clean_text(pdf_path, output_text_path):\n",
    "    print(\"📄 Extracting embedded (structured) PDF text...\")\n",
    "    structured_html = extract_text_from_pdf(pdf_path)\n",
    "    structured_text_lines = extract_plain_text_from_html(structured_html)\n",
    "\n",
    "    print(\"🧠 Running enhanced OCR (TrOCR x2 + EasyOCR x3)...\")\n",
    "    ocr_text_blocks = extract_ocr_from_pdf(pdf_path)\n",
    "\n",
    "    print(\"🔗 Merging embedded and OCR text...\")\n",
    "    merged_lines = merge_texts(structured_text_lines, ocr_text_blocks)\n",
    "\n",
    "    print(\"💾 Saving merged text to file...\")\n",
    "    with open(output_text_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(merged_lines))\n",
    "\n",
    "    print(f\"✅ Final text saved to: {output_text_path}\")\n",
    "    return \"\\n\".join(merged_lines)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a single .txt file containing all text from PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧹 Output folder cleaned.\n",
      "📄 Extracting embedded (structured) PDF text...\n",
      "🧠 Running enhanced OCR (TrOCR x2 + EasyOCR x3)...\n",
      "🔍 Running OCR on page 1...\n",
      "🔍 Running OCR on page 2...\n",
      "🔍 Running OCR on page 3...\n",
      "🔍 Running OCR on page 4...\n",
      "🔍 Running OCR on page 5...\n",
      "🔍 Running OCR on page 6...\n",
      "🔍 Running OCR on page 7...\n",
      "🔍 Running OCR on page 8...\n",
      "🔍 Running OCR on page 9...\n",
      "🔍 Running OCR on page 10...\n",
      "🔍 Running OCR on page 11...\n",
      "🔍 Running OCR on page 12...\n",
      "🔍 Running OCR on page 13...\n",
      "🔍 Running OCR on page 14...\n",
      "🔍 Running OCR on page 15...\n",
      "🔍 Running OCR on page 16...\n",
      "🔍 Running OCR on page 17...\n",
      "🔍 Running OCR on page 18...\n",
      "🔍 Running OCR on page 19...\n",
      "🔍 Running OCR on page 20...\n",
      "🔍 Running OCR on page 21...\n",
      "🔍 Running OCR on page 22...\n",
      "🔍 Running OCR on page 23...\n",
      "🔍 Running OCR on page 24...\n",
      "🔍 Running OCR on page 25...\n",
      "🔍 Running OCR on page 26...\n",
      "🔍 Running OCR on page 27...\n",
      "🔍 Running OCR on page 28...\n",
      "🔗 Merging embedded and OCR text...\n",
      "💾 Saving merged text to file...\n",
      "✅ Final text saved to: /Users/thomasstewart/Desktop/DisabilityLawFirm/OutputData/final_output.txt\n"
     ]
    }
   ],
   "source": [
    "# === INITIALIZE AND RUN PIPELINE ===\n",
    "clean_output_folder(FOLDER_PATH)\n",
    "\n",
    "text = process_pdf_to_clean_text(\n",
    "    pdf_path=PDF_PATH,\n",
    "    output_text_path=RESULT_PATH\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Estimated API Requests Needed: 33\n",
      "⏱️ Wait Time per Request: 100 seconds\n",
      "🕒 Estimated Total Time (waits only): 0.0h 27.0m 30.0s\n",
      "📄 Total HTML Length: 322919 characters\n",
      "📏 Chunk Size Used: 10000 characters\n"
     ]
    }
   ],
   "source": [
    "# === STEP 1: Load HTML ===\n",
    "with open(RESULT_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    html_text = f.read()\n",
    "\n",
    "# === STEP 2: Split into API Chunks ===\n",
    "html_chunks = [html_text[i:i + MAX_CHUNK_SIZE] for i in range(0, len(html_text), MAX_CHUNK_SIZE)]\n",
    "num_requests = len(html_chunks)\n",
    "\n",
    "# === STEP 3: Estimate Time ===\n",
    "total_wait_time = WAIT_TIME_SECONDS * num_requests / api_keys\n",
    "hours, rem = divmod(total_wait_time, 3600)\n",
    "minutes, seconds = divmod(rem, 60)\n",
    "\n",
    "# === STEP 4: Summary Report ===\n",
    "print(f\"📦 Estimated API Requests Needed: {num_requests}\")\n",
    "print(f\"⏱️ Wait Time per Request: {WAIT_TIME_SECONDS} seconds\")\n",
    "print(f\"🕒 Estimated Total Time (waits only): {hours}h {minutes}m {seconds}s\")\n",
    "print(f\"📄 Total HTML Length: {len(html_text)} characters\")\n",
    "print(f\"📏 Chunk Size Used: {MAX_CHUNK_SIZE} characters\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Worker 1:   0%|                                          | 0/17 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Worker 2:   0%|                                          | 0/16 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "Worker 1:   6%|██                                | 1/17 [00:00<00:04,  3.64it/s]\u001b[A\n",
      "\n",
      "Worker 2:   6%|██▏                               | 1/16 [00:16<04:05, 16.39s/it]\u001b[A\u001b[A\n",
      "Worker 1:  12%|███▉                             | 2/17 [03:36<31:53, 127.56s/it]\u001b[A\n",
      "\n",
      "Worker 2:  12%|████▏                            | 2/16 [04:23<35:27, 151.98s/it]\u001b[A\u001b[A\n",
      "Worker 1:  18%|█████▊                           | 3/17 [08:01<44:24, 190.31s/it]\u001b[A\n",
      "\n",
      "Worker 2:  19%|██████▏                          | 3/16 [08:08<40:13, 185.65s/it]\u001b[A\u001b[A\n",
      "Worker 1:  24%|███████▊                         | 4/17 [11:36<43:19, 199.93s/it]\u001b[A\n",
      "\n",
      "Worker 2:  25%|████████▎                        | 4/16 [11:46<39:39, 198.26s/it]\u001b[A\u001b[A\n",
      "Worker 1:  29%|█████████▋                       | 5/17 [15:17<41:27, 207.32s/it]\u001b[A\n",
      "\n",
      "Worker 2:  31%|██████████▎                      | 5/16 [15:22<37:29, 204.53s/it]\u001b[A\u001b[A\n",
      "Worker 1:  35%|███████████▋                     | 6/17 [18:56<38:45, 211.41s/it]\u001b[A\n",
      "\n",
      "Worker 2:  38%|████████████▍                    | 6/16 [19:03<35:03, 210.37s/it]\u001b[A\u001b[A\n",
      "Worker 1:  41%|█████████████▌                   | 7/17 [22:34<35:35, 213.53s/it]\u001b[A\n",
      "\n",
      "Worker 2:  44%|██████████████▍                  | 7/16 [22:46<32:08, 214.31s/it]\u001b[A\u001b[A\n",
      "Worker 1:  47%|███████████████▌                 | 8/17 [26:12<32:13, 214.88s/it]\u001b[A\n",
      "\n",
      "Worker 2:  50%|████████████████▌                | 8/16 [26:32<29:04, 218.11s/it]\u001b[A\u001b[A\n",
      "Worker 1:  53%|█████████████████▍               | 9/17 [29:50<28:47, 215.92s/it]\u001b[A\n",
      "\n",
      "Worker 2:  56%|██████████████████▌              | 9/16 [30:13<25:31, 218.83s/it]\u001b[A\u001b[A\n",
      "\n",
      "Worker 2:  62%|████████████████████            | 10/16 [33:47<21:44, 217.43s/it]\u001b[A\u001b[A\n",
      "Worker 1:  59%|██████████████████▊             | 10/17 [33:54<26:12, 224.62s/it]\u001b[A\n",
      "\n",
      "Worker 2:  69%|██████████████████████          | 11/16 [37:28<18:13, 218.72s/it]\u001b[A\u001b[A\n",
      "Worker 1:  65%|████████████████████▋           | 11/17 [37:34<22:20, 223.38s/it]\u001b[A\n",
      "\n",
      "Worker 2:  75%|████████████████████████        | 12/16 [41:05<14:32, 218.14s/it]\u001b[A\u001b[A\n",
      "Worker 1:  71%|██████████████████████▌         | 12/17 [41:14<18:31, 222.33s/it]\u001b[A\n",
      "Worker 1:  76%|████████████████████████▍       | 13/17 [44:46<14:35, 218.98s/it]\u001b[A\n",
      "\n",
      "Worker 2:  81%|██████████████████████████      | 13/16 [44:50<11:00, 220.24s/it]\u001b[A\u001b[A\n",
      "Worker 1:  82%|██████████████████████████▎     | 14/17 [48:29<11:01, 220.34s/it]\u001b[A\n",
      "\n",
      "Worker 2:  88%|████████████████████████████    | 14/16 [48:33<07:21, 220.97s/it]\u001b[A\u001b[A\n",
      "Worker 1:  88%|████████████████████████████▏   | 15/17 [52:04<07:17, 218.70s/it]\u001b[A\n",
      "\n",
      "Worker 2:  94%|██████████████████████████████  | 15/16 [52:14<03:40, 220.87s/it]\u001b[A\u001b[A\n",
      "Worker 1:  94%|██████████████████████████████  | 16/17 [55:40<03:37, 217.84s/it]\u001b[A\n",
      "\n",
      "Worker 2: 100%|████████████████████████████████| 16/16 [59:09<00:00, 221.86s/it]\u001b[A\u001b[A\n",
      "\n",
      "Worker 1: 100%|██████████████████████████████| 17/17 [1:02:36<00:00, 220.98s/it]\u001b[A\n",
      "\n",
      "📝 Final Bullet-Point Summary:\n",
      "- Client: Christopher Voss\n",
      "  - Birth Date: 11/25/1984\n",
      "  - Address: 1153 East 3900 South, Mountain Land, Salt Lake City, UT: 84124\n",
      "  - Phone Number: 801-262-6331\n",
      "  - Diagnosis: Multiple Sclerosis, Relapsing/Remitting Multiple Sclerosis, Parathesias, chronic pain, muscle spasms, and weakness\n",
      "  - Treatment: Chiropractic care, medical marijuana for sleep, MS meds, infusions, pain medication\n",
      "  - Medications: MS meds, pain medication, medical marijuana\n",
      "  - Allergies: No known drug allergies\n",
      "  - Physicians: Rocky Mountain Multiple Sclerosis Clinic in Salt Lake City, Mark D. Anderson, PT, MPT, OSC\n",
      "  - Therapists: Mark D. Anderson\n",
      "  - Imaging Results: Positive MRI showing lesions consistent with MS, EMG completed for both upper extremities but without known findings\n",
      "  - Job Title: Architect\n",
      "  - Reason for Leaving: Unable to get work done when pain is high or during medication side effects\n",
      "  - Job Duties: Demanding as an Architect with limited mental capacity\n",
      "  - Functional Capacity Evaluation:\n",
      "    - Sitting Tolerance: 25 minutes\n",
      "    - Standing Tolerance: 10 minutes\n",
      "    - Walking Tolerance: 8 minutes\n",
      "    - Lifting Tests:\n",
      "      - Combined Lift/Carry: 15 pounds\n",
      "      - Floor to Waist: 20 pounds\n",
      "      - Waist to Shoulder: 30 pounds\n",
      "      - Overhead: 30 pounds\n",
      "      - Carrying: 10 pounds\n",
      "    - Pushing Test: 50 pounds\n",
      "    - Pulling Test: 50 pounds\n",
      "    - Repetitive Squat Test: 9 times\n",
      "    - Grip Strength: Right: 130 pounds (59.0 kg), Left: 102 pounds (46.3 kg)\n",
      "    - Purdue Pegboard Test: Right Hand: 12.7 Pins, Left Hand: 10 Pins\n",
      "    - Single-Stage Treadmill Walking Test: Estimated VO2max 40.92 ml/kg/min, Average Cardio-Respiratory Fitness Level, 45th Percentile Rank\n",
      "    - Coefficient of Variation (CV) Grip Test: Right hand CV: 2.8%, Left hand CV: 6.95%\n",
      "    - Disabilities of the Arm, Shoulder, and Hand: 41% Perceived Disability\n",
      "    - Lower Extremity Function Scale: 59% Perceived Disability\n",
      "    - QuickDASH: 41% overall disability rating\n",
      "    - QuickDASH Work module: 56% overall disability rating\n",
      "    - QuickDASH Sports/Performing Arts module: 88% overall disability rating\n",
      "    - Functional Capacity: Capable of exerting up to 15 pounds of force occasionally, frequent lifting or carrying objects weighing 8 pounds, not safe performing tasks repetitively over a period of time\n",
      "    - Early Symptoms: Low back pain, numbness in lower extremities starting in Nov 2019\n",
      "    - Recommendations:\n",
      "      - No lifting greater than 20 pounds occasionally from the floor to waist level\n",
      "      - No lifting greater than 30 pounds occasionally from the waist to shoulder level\n",
      "      - No lifting greater than 30 pounds occasionally above the shoulder\n",
      "      - No carrying with both arms more than 10 pounds occasionally at waist level\n",
      "      - No pushing activities exceeding 50 pounds occasionally\n",
      "      - No pulling activities exceeding 50 pounds occasionally\n",
      "      - Occasional (non-repetitive) positional balance activities\n",
      "      - Occasional (non-repetitive) stair climbing activities\n",
      "      - Occasional (non-repetitive) ladder climbing activities\n",
      "      - Frequent (non-repetitive) stooping activities\n",
      "      - Occasional (non-repetitive) kneeling on either knee\n",
      "      - Occasional (non-repetitive) kneeling on both knees simultaneously\n",
      "      - Occasional (non-repetitive) crouching activities\n",
      "      - Occasional (non-repetitive) foot control or movement activities with the right and left feet\n",
      "      - Frequent (repetitive) activities consisting of firm power gripping, holding, or turning objects with the left hand\n",
      "      - Occasional (non-repetitive) activities consisting of simple picking-up, squeezing and pinching small objects with the right and left fingers\n",
      "      - Occasional (non-repetitive) fine motor activities that consist of picking-up small objects with the right and left fingers\n",
      "    - Additional Notes:\n",
      "      - Walking with L side limp, touching TM and equipment for stability\n",
      "      - No R arm swing, focuses with forward gaze to maintain balance\n",
      "      - Neuropathy in feet increasing with time\n",
      "      - Early and building fatigue with compensation in standing, leaning, or avoiding squat\n",
      "      - Lift and exertion with Upper Extremity slowed and displayed a general reduced pace in what presented as building fatigue with activity\n",
      "      - Instabilities, difficulty with balance and coordination during tasks\n",
      "      - Difficulty getting up from floor after kneeling, poor balance\n",
      "      - Fatigue in lower extremities causing sloppy walking and give out when setting down weight\n",
      "      - Good form and reach range of motion at increased weight, no sign of UE or LE changes\n",
      "      - Slow pace to heel raise, but able if holds on, BORG SCALE: Subject's rating of weight (RPE) = 17/20 (very hard)\n",
      "      - L side more fatiguing and LE give out, BORG SCALE: Subject's rating of weight (RPE) = 20/20 (maximal exertion)\n",
      "      - Able to squat into full flexion, but difficult to squat dynamic and fatigue in legs that gives out at 20 sec, increasing symptoms in LE with time, BORG SCALE: Subject's rating of weight (RPE) = 20/20 (maximal exertion)\n",
      "      - Slows pace, fatigue and pain in LE increasing with time and unable to keep up with metronome, braces over knees to recovery, BORG SCALE: Subject's rating of weight (RPE) = 17/20 (very hard), HEART RATE: Max HR = 128\n",
      "      - Avoids bending legs to weight, legs very fatigued, bends over flexing knees, upper body seems better than LE in task, able to lift upper body at 30 Ibs\n",
      "      - Fatigue in both LE cause sloppy walking and give out when setting down weight at 15 Ibs, uses equipment to right himself to stand after carry, unsteady with increased weight\n",
      "      - Only able to push 50 pounds and thus did not meet the demand minimal functional capacity of 100 pounds, limited by pain, decreased mobility and weakness\n",
      "      - Only able to pull 50 pounds and thus did not meet the demand minimal functional capacity of 80 pounds, limited by pain, decreased mobility and weakness\n",
      "📄 Summary saved to: /Users/thomasstewart/Desktop/DisabilityLawFirm/OutputData/api_results_summary.txt\n",
      "✅ DONE\n"
     ]
    }
   ],
   "source": [
    "# === Use f-string to dynamically insert variables into the shell command ===\n",
    "command = f\"\"\"\n",
    "python3 summarize_data.py \\\n",
    "  --input_text \"{RESULT_PATH}\" \\\n",
    "  --output_json \"{OUTPUT_JSON_PATH}\" \\\n",
    "  --output_stats \"{OUTPUT_STATS_PATH}\" \\\n",
    "  --live_output \"{LIVE_OUTPUT_PATH}\" \\\n",
    "  --max_chunk_size {MAX_CHUNK_SIZE} \\\n",
    "  --wait_time {WAIT_TIME_SECONDS}\n",
    "\"\"\"\n",
    "\n",
    "# Run the shell command using !\n",
    "!{command}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Could not count field MedicalHistory.functional_assessments: unhashable type: 'dict'\n",
      "✅ Final consolidated JSON: /Users/thomasstewart/Desktop/DisabilityLawFirm/OutputData/final_consolidated_output.json\n",
      "📊 Field counts saved to: /Users/thomasstewart/Desktop/DisabilityLawFirm/OutputData/field_value_counts.json\n",
      "🩺 Total merged visits: 1\n",
      "📁 Saved section to: /Users/thomasstewart/Desktop/DisabilityLawFirm/JSONdata/Clients.json\n",
      "📁 Saved section to: /Users/thomasstewart/Desktop/DisabilityLawFirm/JSONdata/PersonalIdentifyingInformation.json\n",
      "📁 Saved section to: /Users/thomasstewart/Desktop/DisabilityLawFirm/JSONdata/MedicalHistory.json\n",
      "📁 Saved section to: /Users/thomasstewart/Desktop/DisabilityLawFirm/JSONdata/WorkHistory.json\n",
      "📁 Saved section to: /Users/thomasstewart/Desktop/DisabilityLawFirm/JSONdata/MedicalVisits.json\n",
      "⏳ Waiting 60 seconds before summarizing...\n",
      "✅ Bullet summary saved to: /Users/thomasstewart/Desktop/DisabilityLawFirm/OutputData/final_vector_bullets.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from collections import defaultdict, Counter\n",
    "from mistralai import Mistral\n",
    "\n",
    "# === FILE CONFIGURATION ===\n",
    "RESPONSE_FILE = \"/Users/thomasstewart/Desktop/DisabilityLawFirm/OutputData/api_results.json\"\n",
    "FOLDER_PATH = \"/Users/thomasstewart/Desktop/DisabilityLawFirm/OutputData\"\n",
    "SPLIT_OUTPUT_DIR = \"/Users/thomasstewart/Desktop/DisabilityLawFirm/JSONdata\"\n",
    "\n",
    "FINAL_JSON_PATH = os.path.join(FOLDER_PATH, \"final_consolidated_output.json\")\n",
    "VALUE_COUNT_PATH = os.path.join(FOLDER_PATH, \"field_value_counts.json\")\n",
    "BULLET_PATH = os.path.join(FOLDER_PATH, \"final_vector_bullets.json\")\n",
    "\n",
    "os.makedirs(SPLIT_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# === Load Responses ===\n",
    "with open(RESPONSE_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    responses = json.load(f)\n",
    "\n",
    "MULTI_VALUE_FIELDS = {\n",
    "    \"medications\", \"allergies\", \"therapists\", \"physicians\", \"surgeries\",\n",
    "    \"hospitalizations\", \"diagnosis\", \"lab_results\", \"imaging_results\", \"functional_assessments\",\n",
    "    \"colleagues\", \"job_duties\", \"assets\", \"liabilities\", \"bank_statements\", \"expenses\", \"courses_taken\"\n",
    "}\n",
    "\n",
    "field_values = defaultdict(list)\n",
    "medical_visits = []\n",
    "\n",
    "def flatten_response(response_dict):\n",
    "    flat_data = {}\n",
    "    for section, fields in response_dict.items():\n",
    "        if isinstance(fields, dict):\n",
    "            for key, val in fields.items():\n",
    "                flat_data[f\"{section}.{key}\"] = val\n",
    "    return flat_data\n",
    "\n",
    "def parse_date_safe(date_str):\n",
    "    if not date_str:\n",
    "        return None\n",
    "    for fmt in (\"%Y-%m-%d\", \"%m/%d/%Y\", \"%Y/%m/%d\"):\n",
    "        try:\n",
    "            return datetime.strptime(date_str, fmt)\n",
    "        except:\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "# === Process Responses ===\n",
    "for entry in responses:\n",
    "    try:\n",
    "        content = entry[\"content\"]\n",
    "        parsed = content if isinstance(content, dict) else json.loads(content)\n",
    "\n",
    "        flat = flatten_response(parsed)\n",
    "        for field, value in flat.items():\n",
    "            if value:\n",
    "                key = field.split(\".\")[1]\n",
    "                if key in MULTI_VALUE_FIELDS and isinstance(value, str):\n",
    "                    items = [v.strip() for v in value.split(\",\") if v.strip()]\n",
    "                    field_values[field].extend(items)\n",
    "                else:\n",
    "                    field_values[field].append(value)\n",
    "\n",
    "        for visit in parsed.get(\"MedicalVisits\", []):\n",
    "            if isinstance(visit, dict) and any(visit.values()):\n",
    "                medical_visits.append(visit)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Skipping entry due to error: {e}\")\n",
    "\n",
    "# === Most Common Values ===\n",
    "final_summary = {}\n",
    "for field, values in field_values.items():\n",
    "    try:\n",
    "        normalized = [json.dumps(v, sort_keys=True) if isinstance(v, dict) else str(v) for v in values]\n",
    "        counter = Counter(normalized)\n",
    "        most_common, _ = counter.most_common(1)[0]\n",
    "        final_summary[field] = json.loads(most_common) if most_common.startswith(\"{\") else most_common\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error processing field {field}: {e}\")\n",
    "\n",
    "# === Merge Medical Visits ===\n",
    "visits_by_date = defaultdict(list)\n",
    "for visit in medical_visits:\n",
    "    date = visit.get(\"date\")\n",
    "    if date:\n",
    "        visits_by_date[date].append(visit)\n",
    "\n",
    "merged_visits = []\n",
    "for date, visit_list in visits_by_date.items():\n",
    "    merged = {\n",
    "        \"date\": date,\n",
    "        \"physician\": set(),\n",
    "        \"location\": set(),\n",
    "        \"reason_for_visit\": set(),\n",
    "        \"notes\": set(),\n",
    "        \"recommendations\": set()\n",
    "    }\n",
    "    for visit in visit_list:\n",
    "        for key in merged:\n",
    "            if key != \"date\":\n",
    "                val = visit.get(key)\n",
    "                if val:\n",
    "                    merged[key].add(val.strip())\n",
    "    for key in merged:\n",
    "        if key != \"date\":\n",
    "            merged[key] = \"; \".join(sorted(merged[key])) if merged[key] else None\n",
    "    merged_visits.append(merged)\n",
    "\n",
    "merged_visits = sorted(merged_visits, key=lambda v: parse_date_safe(v[\"date\"]) or datetime.min)\n",
    "final_summary[\"MedicalVisits\"] = merged_visits\n",
    "\n",
    "# === Save Consolidated Output ===\n",
    "with open(FINAL_JSON_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(final_summary, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# === Save Field Value Counts ===\n",
    "value_counts = {}\n",
    "for field, values in field_values.items():\n",
    "    try:\n",
    "        normalized = [json.dumps(v, sort_keys=True) if isinstance(v, dict) else str(v) for v in values]\n",
    "        counts = Counter(normalized)\n",
    "        cleaned_counts = {json.loads(k) if k.startswith(\"{\") else k: v for k, v in counts.items()}\n",
    "        value_counts[field] = cleaned_counts\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Could not count field {field}: {e}\")\n",
    "\n",
    "with open(VALUE_COUNT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(value_counts, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"✅ Final consolidated JSON: {FINAL_JSON_PATH}\")\n",
    "print(f\"📊 Field counts saved to: {VALUE_COUNT_PATH}\")\n",
    "print(f\"🩺 Total merged visits: {len(merged_visits)}\")\n",
    "\n",
    "# === Split Final Summary into Individual JSON Files ===\n",
    "split_jsons = defaultdict(dict)\n",
    "for field, value in final_summary.items():\n",
    "    if \".\" in field:\n",
    "        section, key = field.split(\".\", 1)\n",
    "        split_jsons[section][key] = value\n",
    "    elif field == \"MedicalVisits\":\n",
    "        split_jsons[\"MedicalVisits\"] = value\n",
    "\n",
    "for section, content in split_jsons.items():\n",
    "    section_path = os.path.join(SPLIT_OUTPUT_DIR, f\"{section}.json\")\n",
    "    with open(section_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(content, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"📁 Saved section to: {section_path}\")\n",
    "\n",
    "# === Call Mistral for Bullet Summary ===\n",
    "print(\"⏳ Waiting 60 seconds before summarizing...\")\n",
    "time.sleep(60)\n",
    "\n",
    "with open(FINAL_JSON_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    flattened = f.read()\n",
    "\n",
    "bullet_prompt = f\"\"\"\n",
    "Summarize the following JSON into short, informative bullet points, grouped by category.\n",
    "\n",
    "- Focus on facts\n",
    "- Use one sentence per bullet\n",
    "- No full paragraphs\n",
    "- Designed for embedding into a vector database\n",
    "\n",
    "{flattened}\n",
    "\"\"\"\n",
    "\n",
    "client = Mistral(api_key=os.getenv(\"MISTRAL_API_KEY\"))\n",
    "try:\n",
    "    response = client.chat.complete(\n",
    "        model=\"mistral-large-latest\",\n",
    "        messages=[{\"role\": \"user\", \"content\": bullet_prompt}]\n",
    "    )\n",
    "    bullets = response.choices[0].message.content.strip()\n",
    "except Exception as e:\n",
    "    bullets = f\"[ERROR] Could not summarize: {e}\"\n",
    "\n",
    "with open(BULLET_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"summary_bullets\": bullets}, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"✅ Bullet summary saved to: {BULLET_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Flowise error:\n",
      "Status code: 500\n",
      "Response: {\"statusCode\":500,\"success\":false,\"message\":\"Error: predictionsServices.buildChatflow - null value in column \\\"content\\\" of relation \\\"chat_message\\\" violates not-null constraint\",\"stack\":{}}\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "500 Server Error: Internal Server Error for url: https://flowise.transcendinglegal.com/api/v1/prediction/337f2180-567a-45e7-8916-5b1829663a27",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[78], line 41\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResponse:\u001b[39m\u001b[38;5;124m\"\u001b[39m, response\u001b[38;5;241m.\u001b[39mtext)\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m---> 41\u001b[0m \u001b[43msend_to_flowise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[78], line 39\u001b[0m, in \u001b[0;36msend_to_flowise\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStatus code:\u001b[39m\u001b[38;5;124m\"\u001b[39m, response\u001b[38;5;241m.\u001b[39mstatus_code)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResponse:\u001b[39m\u001b[38;5;124m\"\u001b[39m, response\u001b[38;5;241m.\u001b[39mtext)\n\u001b[0;32m---> 39\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m err\n",
      "Cell \u001b[0;32mIn[78], line 32\u001b[0m, in \u001b[0;36msend_to_flowise\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mpost(API_URL, headers\u001b[38;5;241m=\u001b[39mheaders, json\u001b[38;5;241m=\u001b[39mpayload)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 32\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Flowise response:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28mprint\u001b[39m(response\u001b[38;5;241m.\u001b[39mjson())\n",
      "File \u001b[0;32m/opt/anaconda3/envs/prod/lib/python3.12/site-packages/requests/models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1019\u001b[0m     http_error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1020\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Server Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1021\u001b[0m     )\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 500 Server Error: Internal Server Error for url: https://flowise.transcendinglegal.com/api/v1/prediction/337f2180-567a-45e7-8916-5b1829663a27"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Flowise API\n",
    "API_URL = \"https://flowise.transcendinglegal.com/api/v1/prediction/337f2180-567a-45e7-8916-5b1829663a27\"\n",
    "headers = {\n",
    "    \"Authorization\": \"Bearer Ki6cGg6hvzrq_F6W7MxgAnSSsMikPXiKhmhQaDixCTs\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "# Load your JSON\n",
    "file_path = \"/Users/thomasstewart/Desktop/DisabilityLawFirm/JSONdata/Clients.json\"\n",
    "\n",
    "# === LOAD JSON AND PREPARE PAYLOAD ===\n",
    "def build_payload():\n",
    "    with open(file_path, \"r\") as f:\n",
    "        raw_data = json.load(f)\n",
    "\n",
    "    # Convert full JSON to a clean string\n",
    "    json_string = json.dumps(raw_data, indent=2)\n",
    "\n",
    "    return {\n",
    "        \"content\": f\"{json_string}\\n\\nContext: client medical data\",\n",
    "    }\n",
    "\n",
    "# === SEND TO FLOWISE ===\n",
    "def send_to_flowise():\n",
    "    payload = build_payload()\n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "\n",
    "    try:\n",
    "        response.raise_for_status()\n",
    "        print(\"✅ Flowise response:\")\n",
    "        print(response.json())\n",
    "    except requests.exceptions.HTTPError as err:\n",
    "        print(\"❌ Flowise error:\")\n",
    "        print(\"Status code:\", response.status_code)\n",
    "        print(\"Response:\", response.text)\n",
    "        raise err\n",
    "\n",
    "send_to_flowise()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
